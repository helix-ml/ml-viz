{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code from https://medium.com/district-data-labs/building-a-classifier-from-census-data-18f996c4d7cf\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, log_loss\n",
    "import xgboost as xgb\n",
    "\n",
    "import mlflow\n",
    "import mlflow.xgboost\n",
    "import mlflow.sklearn\n",
    "\n",
    "# prepare train and test data\n",
    "names = [\n",
    "    'age',\n",
    "    'workclass',\n",
    "    'fnlwgt',\n",
    "    'education',\n",
    "    'education-num',\n",
    "    'marital-status',\n",
    "    'occupation',\n",
    "    'relationship',\n",
    "    'race',\n",
    "    'sex',\n",
    "    'capital-gain',\n",
    "    'capital-loss',\n",
    "    'hours-per-week',\n",
    "    'native-country',\n",
    "    'income',\n",
    "]\n",
    "data = pd.read_csv('census_data/adult.data', names=names)\n",
    "\n",
    "import json\n",
    "meta = {\n",
    "    'target_names': list(data.income.unique()),\n",
    "    'feature_names': list(data.columns),\n",
    "    'categorical_features': {\n",
    "        column: list(data[column].unique())\n",
    "        for column in data.columns\n",
    "        if data[column].dtype == 'object'\n",
    "    },\n",
    "}\n",
    "with open('census_data/meta.json', 'w') as f:\n",
    "    json.dump(meta, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets.base import Bunch\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "def load_data():\n",
    "    # Load the meta data from the file\n",
    "    with open('census_data/meta.json', 'r') as f:\n",
    "        meta = json.load(f)\n",
    "    names = meta['feature_names']\n",
    "    # Load the training and test data, skipping the bad row in the test data\n",
    "    train = pd.read_csv('census_data/adult.data', names=names)\n",
    "    test  = pd.read_csv('census_data/adult.test', names=names, skiprows=1)\n",
    "    # Remove the target from the categorical features\n",
    "    meta['categorical_features'].pop('income')\n",
    "    # Return the bunch with the appropriate data chunked apart\n",
    "    return Bunch(\n",
    "        data = train[names[:-1]],\n",
    "        target = train[names[-1]],\n",
    "        data_test = test[names[:-1]],\n",
    "        target_test = test[names[-1]],\n",
    "        target_names = meta['target_names'],\n",
    "        feature_names = meta['feature_names'],\n",
    "        categorical_features = meta['categorical_features'],\n",
    "        DESCR = \"descr\",\n",
    "    )\n",
    "dataset = load_data()\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "class EncodeCategorical(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Encodes a specified list of columns or all columns if None.\n",
    "    \"\"\"\n",
    "    def __init__(self, columns=None):\n",
    "        self.columns  = columns\n",
    "        self.encoders = None\n",
    "    def fit(self, data, target=None):\n",
    "        \"\"\"\n",
    "        Expects a data frame with named columns to encode.\n",
    "        \"\"\"\n",
    "        # Encode all columns if columns is None\n",
    "        if self.columns is None:\n",
    "            self.columns = data.columns\n",
    "        # Fit a label encoder for each column in the data frame\n",
    "        self.encoders = {\n",
    "            column: LabelEncoder().fit(data[column])\n",
    "            for column in self.columns\n",
    "        }\n",
    "        return self\n",
    "    def transform(self, data):\n",
    "        \"\"\"\n",
    "        Uses the encoders to transform a data frame.\n",
    "        \"\"\"\n",
    "        output = data.copy()\n",
    "        for column, encoder in self.encoders.items():\n",
    "            output[column] = encoder.transform(data[column])\n",
    "        return output\n",
    "encoder = EncodeCategorical(dataset.categorical_features.keys())\n",
    "dataset.data = encoder.fit_transform(dataset.data)\n",
    "dataset.data_test = encoder.fit_transform(dataset.data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "import numpy as np\n",
    "\n",
    "class ImputeCategorical(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Encodes a specified list of columns or all columns if None.\n",
    "    \"\"\"\n",
    "    def __init__(self, columns=None):\n",
    "        self.columns = columns\n",
    "        self.imputer = None\n",
    "    def fit(self, data, target=None):\n",
    "        \"\"\"\n",
    "        Expects a data frame with named columns to impute.\n",
    "        \"\"\"\n",
    "        # Encode all columns if columns is None\n",
    "        if self.columns is None:\n",
    "            self.columns = data.columns\n",
    "        # Fit an imputer for each column in the data frame\n",
    "        self.imputer = SimpleImputer(missing_values=0, strategy='most_frequent')\n",
    "        self.imputer.fit(data[self.columns])\n",
    "        return self\n",
    "    def transform(self, data):\n",
    "        \"\"\"\n",
    "        Uses the encoders to transform a data frame.\n",
    "        \"\"\"\n",
    "        output = data.copy()\n",
    "        output[self.columns] = self.imputer.transform(output[self.columns])\n",
    "        return output\n",
    "imputer = ImputeCategorical(['workclass', 'native-country', 'occupation'])\n",
    "dataset.data = imputer.fit_transform(dataset.data)\n",
    "dataset.data_test = imputer.fit_transform(dataset.data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = dataset.data\n",
    "yencode = LabelEncoder().fit(dataset.target)\n",
    "y_train = yencode.transform(dataset.target)\n",
    "\n",
    "X_test = dataset.data_test\n",
    "y_test = yencode.transform([y.rstrip(\".\") for y in dataset.target_test])\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "dtest = xgb.DMatrix(X_test, label=y_test)\n",
    "\n",
    "# enable auto logging\n",
    "mlflow.xgboost.autolog()\n",
    "\n",
    "def grid_test_xgboost(colsample_tree, subsample, max_depth, min_child_weight, eta):\n",
    "    with mlflow.start_run():\n",
    "        # train model\n",
    "        params = {\n",
    "            'objective': 'multi:softprob',\n",
    "            'num_class': 2,\n",
    "            'eval_metric': 'mlogloss',\n",
    "            'max_depth': max_depth,\n",
    "            'min_child_weight': min_child_weight,\n",
    "            'eta':eta,\n",
    "            'subsample': subsample,\n",
    "            'colsample_bytree': colsample_tree\n",
    "        }\n",
    "        model = xgb.train(params, dtrain, evals=[(dtrain, 'train')])\n",
    "\n",
    "        # evaluate model\n",
    "        y_proba = model.predict(dtest)\n",
    "        y_pred = y_proba.argmax(axis=1)\n",
    "        loss = log_loss(y_test, y_proba)\n",
    "        acc = accuracy_score(y_test, y_pred)\n",
    "\n",
    "        # log metrics\n",
    "        mlflow.log_metrics({'log_loss': loss, 'accuracy': acc})\n",
    "        return acc\n",
    "    \n",
    "from sklearn.svm import SVC\n",
    "def grid_test_svm(kernel, gamma, C):\n",
    "    with mlflow.start_run():\n",
    "        clf = SVC(kernel=kernel, gamma=gamma, C=C).fit(X_train, y_train)\n",
    "        accuracy = clf.score(X_test, y_test)\n",
    "        mlflow.log_metrics({'accuracy': accuracy})\n",
    "        mlflow.log_params({'kernel': kernel, 'gamma': gamma, 'C': C})\n",
    "        return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid Search XGBoost and Sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#values from https://blog.cambridgespark.com/hyperparameter-tuning-in-xgboost-4ff9100a3b2f\n",
    "colsample_tree = [0.2, 0.5, 1.0]\n",
    "subsample = [0.05, 1.0]\n",
    "max_depth = [1, 5, 10]\n",
    "min_child_weight = [6]\n",
    "eta = [.9, .3, .01, .005]\n",
    "\n",
    "for i in colsample_tree:\n",
    "    for j in subsample:\n",
    "        for k in max_depth:\n",
    "            for l in min_child_weight:\n",
    "                for m in eta:\n",
    "                    grid_test_xgboost(i, j, k, l, m)\n",
    "\n",
    "kernel = ['rbf']\n",
    "gamma = [1e-3, 1e-4]\n",
    "C = [1, 10, 100, 1000]\n",
    "for i in kernel:\n",
    "    for j in gamma:\n",
    "        for k in C:\n",
    "            grid_test_svm(i, j, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-mlogloss:0.65907\n",
      "[1]\ttrain-mlogloss:0.63675\n",
      "[2]\ttrain-mlogloss:0.60855\n",
      "[3]\ttrain-mlogloss:0.59430\n",
      "[4]\ttrain-mlogloss:0.57516\n",
      "[5]\ttrain-mlogloss:0.55993\n",
      "[6]\ttrain-mlogloss:0.54759\n",
      "[7]\ttrain-mlogloss:0.53285\n",
      "[8]\ttrain-mlogloss:0.52194\n",
      "[9]\ttrain-mlogloss:0.51432\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/mlflow/xgboost.py:333: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  all_arg_names = inspect.getargspec(original)[0]  # pylint: disable=W1505\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7767950371598796\n"
     ]
    }
   ],
   "source": [
    "print(grid_test(0.05, 0.05, 10, 10, 0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "dirs = os.listdir(\"mlruns/0\")\n",
    "dictionary = []\n",
    "for i in dirs:\n",
    "    row = {}\n",
    "    if \"meta\" in i or \"DS_Store\" in i:\n",
    "        continue\n",
    "    accuracy = float(open('mlruns/0/' + i + '/metrics/accuracy').read().split(\" \")[1])\n",
    "    row['accuracy'] = accuracy\n",
    "    row['model'] = 'xgboost'\n",
    "    params_files = os.listdir('mlruns/0/' + i + '/params')\n",
    "    model_params = {}\n",
    "    for j in params_files:\n",
    "        if 'kernel' in j:\n",
    "            continue\n",
    "        model_params[j] = open('mlruns/0/' + i + '/params/' + j).read()\n",
    "    \n",
    "    row['model_params'] = model_params\n",
    "    dictionary.append(row)\n",
    "    \n",
    "df = pd.DataFrame(dictionary)\n",
    "df.to_csv('mlflow_workflows.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
